---
title: "Class09mini"
author: 'San Luc (PID: A59010657)'
date: "10/27/2021"
output: 
     github_document
---

# Exploratory data analysis


## Preparing the data

```{r}
fna.data <- "WisconsinCancer.csv"

wisc.df <- read.csv(fna.data,row.names = 1)
```

### Examine your input data to ensure column names are set correctly. The id and diagnosis columns will not be used for most of the following steps (you can use the View() or head() functions here).

```{r}
View(wisc.df,)
head(wisc.df, 1)
```

### Note that the first column here wisc.df$diagnosis is a pathologist provided expert diagnosis. We will not be using this for our unsupervised analysis as it is essentially the “answer” to the question which cell samples are malignant or benign.

### To make sure we don’t accidentally include this in our analysis, lets create a new data.frame that omits this first column

```{r}
wisc.data <- wisc.df[,-1]
```

### Finally, setup a separate new vector called diagnosis that contains the data from the diagnosis column of the original dataset. We will store this as a factor (useful for plotting) and use this later to check our results.

```{r}
diagnosis <- as.factor(wisc.df$diagnosis)
diagnosis

```

# Exploratory data analysis
The first step of any data analysis, unsupervised or supervised, is to familiarize yourself with the data.

### Explore the data you created before (wisc.data and diagnosis) to answer the following questions:

Q1. How many observations are in this dataset?

```{r}
nrow(wisc.data)
```
569 

Q2. How many of the observations have a malignant diagnosis?

```{r}
table(diagnosis)
```
212

Q3. How many variables/features in the data are suffixed with _mean?
```{r}
length(grep("_mean", colnames(wisc.data)))
```
10


# Principal Component Analysis

The next step in your analysis is to perform principal component analysis (PCA) on wisc.data.

It is important to check if the data need to be scaled before performing PCA. Recall two common reasons for scaling data include:

The input variables use different units of measurement.
The input variables have significantly different variances.
Check the mean and standard deviation of the features (i.e. columns) of the wisc.data to determine if the data should be scaled. Use the colMeans() and apply() functions like you’ve done before.


```{r}
colMeans(wisc.data)
apply(wisc.data,2,sd)

```

Execute PCA with the prcomp() function on the wisc.data, scaling if appropriate, and assign the output model to wisc.pr.
```{r}
wisc.data <- wisc.data[,-31]
wisc.pr <- prcomp(wisc.data, scale = TRUE)
```

Inspect a summary of the results with the summary() function.

```{r}
summary(wisc.pr)
```

Q4. From your results, what proportion of the original variance is captured by the first principal components (PC1)?
44%

Q5. How many principal components (PCs) are required to describe at least 70% of the original variance in the data?
three. PC1-PC3

Q6. How many principal components (PCs) are required to describe at least 90% of the original variance in the data?
7 PC1-PC7


Q7. What stands out to you about this plot? Is it easy or difficult to understand? Why?
it's so hard to understand. I honestly almost cannot see anything from the plot. What we can see is that there are PC1 and PC2 are plot onto the coordinates and aremore abundance in one area than the other. 

```{r}
biplot(wisc.pr)
```

# Scatter plot observations by components 1 and 2
```{r}
plot( wisc.pr$x[,1:2], col = diagnosis, 
     xlab = "PC1", ylab = "PC2")
```

Q8.Generate a similar plot for principal components 1 and 3. What do you notice about these plots?
```{r}
plot(wisc.pr$x[, 1:3], col = diagnosis, 
     xlab = "PC1", ylab = "PC3")
```
the graph above isn't right so i did something worng with my code here. However, the graph given in the assignment shows that PC1 and PC3 as components doesnt provide clearer data and cluster then the one above. 

### Because principal component 2 explains more variance in the original data than principal component 3, you can see that the first plot has a cleaner cut separating the two subgroups.


As this is such a striking result let’s see if we can use the ggplot2 package to make a more fancy figure of these results. 
# Create a data.frame for ggplot
```{r}
df <- as.data.frame(wisc.pr$x)
df$diagnosis <- diagnosis
```

# Load the ggplot2 package
```{r}
library(ggplot2)
```

# Make a scatter plot colored by diagnosis

```{r}
ggplot(df) + 
  aes(PC1, PC2, col = diagnosis) + 
  geom_point()
```

Variance explained

Calculate the variance of each principal component by squaring the sdev component of wisc.pr
# Calculate variance of each component

```{r}
pr.var <- wisc.pr$sdev^2
head(pr.var)
```


# Variance explained by each principal component: pve

```{r}
pve <- pr.var/sum(pr.var)
```

# Plot variance explained for each principal component

```{r}
plot(pve, xlab = "Principal Component", 
     ylab = "Proportion of Variance Explained", 
     ylim = c(0, 1), type = "o")
```


```{r}
barplot(pve, ylab = "Precent of Variance Explained",
     names.arg=paste0("PC",1:length(pve)), las=2, axes = FALSE)
axis(2, at=pve, labels=round(pve,2)*100 )
```

```{r}
library(factoextra)
fviz_eig(wisc.pr, addlabels = TRUE)
```



Communicating PCA results
Q9. For the first principal component, what is the component of the loading vector (i.e. wisc.pr$rotation[,1]) for the feature concave.points_mean?
```{r}
wisc.pr$rotation[,1]
```
-0.261 

Q10. What is the minimum number of principal components required to explain 80% of the variance of the data?
5, from the summary table

3. Hierarchical clustering

# First scale the wisc.data data and assign the result to data.scaled.

## Scale the wisc.data data using the "scale()" function
```{r}
data.scaled <- scale(wisc.data)
```

Calculate the (Euclidean) distances between all pairs of observations in the new scaled dataset and assign the result to data.dist.

```{r}
data.dist <- dist(data.scaled,"euclidean")
```


Create a hierarchical clustering model using complete linkage. Manually specify the method argument to hclust() and assign the results to wisc.hclust.

```{r}
wisc.hclust <- hclust(data.dist, "complete")
```

Results of hierarchical clustering

Q11. Using the plot() and abline() functions, what is the height at which the clustering model has 4 clusters?
```{r}
plot(wisc.hclust)
abline(wisc.hclust, col= "Red", lty=2)
```
I'm not sure why the red line isnt showing up... but it seems like at around 20 where the plot has 4 clusters. 

Selecting number of clusters

# This exercise will help you determine if, in this case, hierarchical clustering provides a promising new feature.

## Use cutree() to cut the tree so that it has 4 clusters. Assign the output to the variable wisc.hclust.clusters.

```{r}
wisc.hclust.clusters <- cutree(wisc.hclust, k = 4)
```

We can use the table() function to compare the cluster membership to the actual diagnoses.

```{r}
table(wisc.hclust.clusters, diagnosis)
```

Before moving on, explore how different numbers of clusters affect the ability of the hierarchical clustering to separate the different diagnoses.

Q12. Can you find a better cluster vs diagnoses match by cutting into a different number of clusters between 2 and 10?

I don't think so, because we biologically have two diagnosis and its false negative/false positive. I might be interpreting this wrong though... 


# Using different methods

As we discussed in our last class videos there are number of different “methods” we can use to combine points during the hierarchical clustering procedure. These include "single", "complete", "average" and (my favorite) "ward.D2".

Q13. Which method gives your favorite results for the same data.dist dataset? Explain your reasoning.
Based on the explanation of ward.D2, I think I like ward.D2 the most, since they said to have an effect for looking from individual clusters and slowly merge them into a big group, and for our data set on cancer cell, i think it will start broad so all clusters are merge inclusively. 


# Combining methods

Clustering on PCA results
```{r}
plot(wisc.pr$x[ ,1:2], col = diagnosis) 
```
```{r}
summary(wisc.pr)
```



I will use 4PCs and 'hclust()' and dist()

```{r}
wisc.pr.hclust <- hclust(dist(wisc.pr$x[,1:4]), method = "ward.D2")
```
```{r}
plot(wisc.pr.hclust)
abline(h=80, col= "Red")
```



This looks much more promising than our previous clustering results on the original scaled data. Note the two main branches of or dendrogram indicating two main clusters - maybe these are malignant and benign. Let’s find out!

```{r}
grps <- cutree(wisc.pr.hclust, k=2)
table(grps)
```

```{r}
table(grps, diagnosis)
```
```{r}
table(grps, diagnosis)
```
```{r}
plot(wisc.pr$x[,1:2], col=grps)
```

```{r}
plot(wisc.pr$x[,1:2], col=diagnosis)
```

## Use the distance along the first 7 PCs for clustering i.e. wisc.pr$x[, 1:7]
```{r}
wisc.pr.hclust <- hclust(data.dist, method="ward.D2")
plot(wisc.pr.hclust)
```
```{r}
wisc.pr.hclust.clusters <- cutree(wisc.pr.hclust, k=2)

# Compare to actual diagnoses
table(wisc.pr.hclust.clusters, diagnosis)
```




**Accuracy**, essentially how much did we get correct? 

```{r}
(165+351)/nrow(wisc.data)
```

**Sensitivity** = True Positive/ (True positive+False Negative)
```{r}
(165)/(6 + 165)
```
**Specificity** = True Negative/ (True Negative+False Negative)
```{r}
(351)/(351+47)
```

Q17. Which of your analysis procedures resulted in a clustering model with the best specificity? How about sensitivity?


```{r}
plot(wisc.pr$x[,1:2], col=grps)
```

```{r}
plot(wisc.pr$x[,1:2], col=diagnosis)
```

OPTIONAL: Note the color swap here as the hclust cluster 1 is mostly “M” and cluster 2 is mostly “B” as we saw from the results of calling table(grps, diagnosis). To match things up we can turn our groups into a factor and reorder the levels so cluster 2 comes first and thus gets the first color (black) and cluster 1 gets the second color (red).

# Prediction
We will use the predict() function that will take our PCA model from before and new cancer cell data and project that data onto our PCA space.

#url <- "new_samples.csv"

```{r}
url <- "https://tinyurl.com/new-samples-CSV"
new <- read.csv(url)
npc <- predict(wisc.pr, newdata=new)
npc
```

### Plot
```{r}
plot(wisc.pr$x[,1:2], col= diagnosis)
points(npc[,1], npc[,2], col="blue", pch=16, cex=3)
text(npc[,1], npc[,2], c(1,2), col="white")
```

Q18. Which of these new patients should we prioritize for follow up based on your results?
patient 2 should be prioritized for follow up, since their prediction are in the malignant group. 







